import requests
from bs4 import BeautifulSoup
import re
import os

WEBHOOK_URL = os.environ.get("WEBHOOK_URL")

def fetch_current_events():
    url = "https://bluearchive.wikiru.jp/"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    response = requests.get(url, headers=headers)
    response.encoding = 'utf-8'

    if response.status_code != 200:
        raise Exception(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")

    soup = BeautifulSoup(response.text, 'html.parser')

    # ã€Œé–‹å‚¬ä¸­ã®ã‚¤ãƒ™ãƒ³ãƒˆã€ã¨ã„ã†æ–‡å­—ã‚’å«ã‚€ <p> ã‚¿ã‚°ã‚’æ¢ã™
    target_p = None
    for p in soup.find_all("p"):
        if "ç¾åœ¨é–‹å‚¬ä¸­ã®ã‚¤ãƒ™ãƒ³ãƒˆ" in p.get_text():
            target_p = p
            break

    if not target_p:
        raise Exception("é–‹å‚¬ä¸­ã®ã‚¤ãƒ™ãƒ³ãƒˆã®è¦‹å‡ºã—ï¼ˆpã‚¿ã‚°ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # <p>ã®æ¬¡ã«æ¥ã‚‹<ul>ã‚¿ã‚°ã‚’å–å¾—ï¼
    event_ul = target_p.find_next_sibling("ul")
    if not event_ul:
        raise Exception("ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§ã®ulã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    events = []
    for li in event_ul.find_all("li"):
        text = li.get_text(strip=True)
        match = re.match(r"(.*?)(\d{4}/\d{1,2}/\d{1,2}.*)", text)
        if match:
            name = match.group(1).strip()
            period = match.group(2).strip()
            events.append(f"{name}ï¼š{period}")
        else:
            # æ—¥ä»˜ãŒãªãã¦ã‚‚ä¸€å¿œè¿½åŠ ã—ã¦ã¿ã‚‹
            events.append(text)

    return events

def main():
    events = fetch_current_events()
    if not events:
        content = "ç¾åœ¨ã€é–‹å‚¬ä¸­ã®ã‚¤ãƒ™ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    else:
        content = "ğŸ“¢ é–‹å‚¬ä¸­ã®ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ï¼š\n" + "\n".join(events)

    response = requests.post(WEBHOOK_URL, json={"content": content})
    if response.status_code != 204:
        raise Exception(f"é€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}, ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")

if __name__ == "__main__":
    main()
